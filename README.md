# Preparaci√≥n de entorno:

## Instalaci√≥n de dependencias
```bash
pip install -r requirements.txt
```
# Fase 1: Data Ingestion (Generaci√≥n y Organizaci√≥n de Datos)

## üìã Descripci√≥n general

En esta primera fase se configur√≥ la base del *Data Lake local* y se ejecut√≥ el proceso de generaci√≥n autom√°tica de transacciones.  
El objetivo fue establecer una estructura organizada para la ingesta continua de datos y preparar el entorno para las siguientes fases del pipeline ETL.

El archivo `main.py` se encarga de generar archivos CSV con transacciones simuladas cada 60 segundos dentro de la carpeta `transactions/`.

Durante esta fase:
- Se ejecut√≥ `main.py` para verificar la creaci√≥n autom√°tica de los archivos CSV.

## Ejecuci√≥n
```bash
python3 main.py
```

- Se verific√≥ que los archivos CSV fueran generados correctamente por main.py y que su estructura (columnas y formato) fuera consistente entre los distintos archivos.


### üìã Inspecci√≥n general de archivos

El script `inspect_transactions.py` permite analizar los archivos CSV generados en la carpeta `transactions/`.  
Este script realiza validaciones b√°sicas, como verificar la consistencia de las columnas y detectar posibles errores en los datos.

### Ejecuci√≥n
```bash
python3 inspect_transactions.py
```

- El script genera un reporte con los resultados de la inspecci√≥n, indicando si los archivos cumplen con los est√°ndares definidos.


# Fase 2. ETL Pipeline - Limpieza y Detecci√≥n de Fraude

Creaci√≥n y aplicaci√≥n de funciones:

## Funci√≥n clean_data(df): 
* Limpia y valida el DataFrame de transacciones crudas.
* Elimina valores nulos y duplicados en las columnas cr√≠ticas.
* Estandariza formatos de texto (may√∫sculas, sin espacios) en campos clave como currency, pa√≠s, etc.
* Convierte tipos de datos (num√©ricos y fechas) correctamente.
* Maneja outliers en la columna amount usando el m√©todo IQR.
* Prepara los datos para la detecci√≥n de fraude, pero no guarda el DataFrame limpio directamente; lo pasa a la siguiente etapa del pipeline.

**Decisi√≥n sobre la limpieza de datos**.
Durante la fase de limpieza de datos, se evaluaron dos enfoques:

**Limpieza de todas las columnas:** 

 **Ventaja:** Garantiza m√°xima calidad y consistencia en todos los campos.

**Desventaja:** Puede eliminar una gran cantidad de registros por valores faltantes o inconsistentes en columnas poco relevantes, reduciendo el volumen de datos √∫til para an√°lisis y detecci√≥n de fraude.

Datos totales despu√©s de limpieza:

* Cantidad de filas originales: 100
* Cantidad de filas despu√©s de limpieza: 54


**Limpieza solo de columnas cr√≠ticas:**

**Ventaja:** Preserva m√°s registros, enfoc√°ndose en los campos esenciales para el an√°lisis y la detecci√≥n de transacciones sospechosas.

**Desventaja:** Puede dejar inconsistencias menores en columnas no cr√≠ticas, pero no afecta la calidad del an√°lisis principal.

**Decisi√≥n tomada:**
Se opt√≥ por limpiar √∫nicamente las columnas cr√≠ticas para el proceso de an√°lisis y detecci√≥n de fraude:

* transaction_id
* user_id
* merchant_id
* amount
* currency
* status
* timestamp
* payment_method
* country

Esta decisi√≥n permite mantener la mayor cantidad de datos posible, asegurando la calidad en los campos relevantes y facilitando la detecci√≥n de patrones sospechosos sin sacrificar volumen de informaci√≥n.

## Justificaci√≥n num√©rica:

Haciendo limpieza de columnas cr√≠ticas los resultados son mejores: 
* Cantidad de filas originales: 100
* Cantidad de filas despu√©s de limpieza: 93

## Funci√≥n detect_suspicious_transactions(df):

* Recibe el DataFrame limpio y detecta transacciones sospechosas seg√∫n criterios definidos.

**Criterios principales:**

* Montos inusualmente altos (percentil 99).
* M√∫ltiples intentos fallidos del mismo usuario.
* Flaggeo de transacciones declined con c√≥digos de seguridad (usando tanto response_message que contenga ‚Äúsecurity‚Äù como response_code igual a 65).
* Patrones an√≥malos (transacciones r√°pidas, horarios inusuales).
* Transacciones internacionales de alto riesgo.

**Nota:**
Lo m√©todos estad√≠sticos aplicados y las regflas basadas en patrones t√≠picos cumplen con lo solicitado. 
Para alcanzar un nivel de optimizaci√≥n superior y una calibraci√≥n din√°mica de los criterios, ser√≠a recomendable desarrollar y evaluar un modelo de machine learning supervisado.
No obstante, este procedimiento excede el alcance y las instrucciones del reto actual, por lo que la soluci√≥n presentada prioriza robustez y flexibilidad mediante reglas basadas en el an√°lisis estad√≠stico de los datos.

* La funci√≥n retorna dos DataFrames:
**normal_df:** transacciones normales, que se guardan en la carpeta **processed**. Y el DataFrame
**suspicious_df:** transacciones sospechosas, que se guardan en la carpeta **suspicious**.

## Testeo de funciones:
La carpeta scripts contiene dos archivos: 
* **test_clean.py** que prueba la funci√≥n clean_data. 
* **test_suspicious.py** que prueba la funci√≥n detect_suspicious_transactions. 

### Fase 3: Data Warehouse - Modelado y Almacenamiento
**Objetivo:** Dise√±ar e implementar un modelo dimensional para an√°lisis

* **inspect_processed.py**: Este script permite inspeccionar los archivos CSV limpios generados por el pipeline ETL y almacenados en la carpeta **processed**. Su objetivo es mostrar las columnas disponibles, los tipos de datos y la cantidad de valores nulos en los datos finales, facilitando el dise√±o del modelo dimensional y la creaci√≥n de las tablas en el Data Warehouse.

**Uso recomendado:**
Ejecuta este script antes de definir el esquema de las tablas en PostgreSQL para asegurarte de que el modelo dimensional se adapte a los datos realmente disponibles.

Comando de ejecuci√≥n:

```bash
python scripts/inspect_processed.py