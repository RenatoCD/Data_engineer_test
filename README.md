# Parte 1

## Instalaci贸n de dependencias
```bash
pip install -r requirements.txt
```
# Fase 1: Data Ingestion (Generaci贸n y Organizaci贸n de Datos)

##  Descripci贸n general

En esta primera fase se configur贸 la base del *Data Lake local* y se ejecut贸 el proceso de generaci贸n autom谩tica de transacciones.  
El objetivo fue establecer una estructura organizada para la ingesta continua de datos y preparar el entorno para las siguientes fases del pipeline ETL.

El archivo `main.py` se encarga de generar archivos CSV con transacciones simuladas cada 60 segundos dentro de la carpeta `transactions/`.

Durante esta fase:
- Se ejecut贸 `main.py` para verificar la creaci贸n autom谩tica de los archivos CSV.

## Ejecuci贸n
```bash
python3 main.py
```

- Se verific贸 que los archivos CSV fueran generados correctamente por main.py y que su estructura (columnas y formato) fuera consistente entre los distintos archivos.

## Fase 2: Inspecci贸n de Transacciones

###  Descripci贸n general

El script `inspect_transactions.py` permite analizar los archivos CSV generados en la carpeta `transactions/`.  
Este script realiza validaciones b谩sicas, como verificar la consistencia de las columnas y detectar posibles errores en los datos.

### Ejecuci贸n
```bash
python3 inspect_transactions.py
```

- El script genera un reporte con los resultados de la inspecci贸n, indicando si los archivos cumplen con los est谩ndares definidos.



