# Pipeline de datos / Data Engineering 

## Evaluaci√≥n T√©cnica Integral

**Autor:** [Yuri D√≠az]  

### Descripci√≥n

El objetivo de este repositorio es dise√±ar e implementar un pipeline de datos end-to-end para una fintech latinoamericana, abarcando desde la ingesta y limpieza de datos hasta el modelado dimensional y la detecci√≥n de fraude.

A continuaci√≥n se documentan las fases completadas, las decisiones t√©cnicas tomadas y el razonamiento detr√°s de cada etapa del desarrollo.

---

# Preparaci√≥n de entorno virtual:


Crear un entorno virtual de trabajo utilizando los siguientes comandos:

**py -m venv venv** 

 Otras opciones en lugar de **py** pueden ser **python** o **python3.**

Activamos el entorno con uno de estos tres posibles comandos:

1. source venv/Scripts/activate
2. source venv\Scripts\activate
3. source venv/bin/activate | Linux y Mac


## Instalaci√≥n de dependencias
```bash
pip install -r requirements.txt
```
# Fase 1: Data Ingestion (Generaci√≥n y Organizaci√≥n de Datos)

## üìã Descripci√≥n general

En esta primera fase se configur√≥ la base del *Data Lake local* y se ejecut√≥ el proceso de generaci√≥n autom√°tica de transacciones.  
El objetivo fue establecer una estructura organizada para la ingesta continua de datos y preparar el entorno para las siguientes fases del pipeline ETL.

El archivo `main.py` se encarga de generar archivos CSV con transacciones simuladas cada 60 segundos dentro de la carpeta `transactions/`.

Durante esta fase:
- Se ejecut√≥ `main.py` para verificar la creaci√≥n autom√°tica de los archivos CSV.

## Ejecuci√≥n
```bash
python3 main.py
```

- Se verific√≥ que los archivos CSV fueran generados correctamente por main.py y que su estructura (columnas y formato) fuera consistente entre los distintos archivos.


### üìã Inspecci√≥n general de archivos

El script `inspect_transactions.py` permite analizar los archivos CSV generados en la carpeta `transactions/`.  
Este script realiza validaciones b√°sicas, como verificar la consistencia de las columnas y detectar posibles errores en los datos.

### Ejecuci√≥n
```bash
python3 inspect_transactions.py
```

- El script genera un reporte con los resultados de la inspecci√≥n, indicando si los archivos cumplen con los est√°ndares definidos.


# Fase 2. ETL Pipeline - Limpieza y Detecci√≥n de Fraude

Creaci√≥n y aplicaci√≥n de funciones:

## Funci√≥n clean_data(df): 
* Limpia y valida el DataFrame de transacciones crudas.
* Elimina valores nulos y duplicados en las columnas cr√≠ticas.
* Estandariza formatos de texto (may√∫sculas, sin espacios) en campos clave como currency, pa√≠s, etc.
* Convierte tipos de datos (num√©ricos y fechas) correctamente.
* Maneja outliers en la columna amount usando el m√©todo IQR.
* Prepara los datos para la detecci√≥n de fraude, pero no guarda el DataFrame limpio directamente; lo pasa a la siguiente etapa del pipeline.

**Decisi√≥n sobre la limpieza de datos**.
Durante la fase de limpieza de datos, se evaluaron dos enfoques:

**Limpieza de todas las columnas:** 

 **Ventaja:** Garantiza m√°xima calidad y consistencia en todos los campos.

**Desventaja:** Puede eliminar una gran cantidad de registros por valores faltantes o inconsistentes en columnas poco relevantes, reduciendo el volumen de datos √∫til para an√°lisis y detecci√≥n de fraude.

Datos totales despu√©s de limpieza:

* Cantidad de filas originales: 100
* Cantidad de filas despu√©s de limpieza: 54


**Limpieza solo de columnas cr√≠ticas:**

**Ventaja:** Preserva m√°s registros, enfoc√°ndose en los campos esenciales para el an√°lisis y la detecci√≥n de transacciones sospechosas.

**Desventaja:** Puede dejar inconsistencias menores en columnas no cr√≠ticas, pero no afecta la calidad del an√°lisis principal.

**Decisi√≥n tomada:**
Se opt√≥ por limpiar √∫nicamente las columnas cr√≠ticas para el proceso de an√°lisis y detecci√≥n de fraude:

* transaction_id
* user_id
* merchant_id
* amount
* currency
* status
* timestamp
* payment_method
* country

Esta decisi√≥n permite mantener la mayor cantidad de datos posible, asegurando la calidad en los campos relevantes y facilitando la detecci√≥n de patrones sospechosos sin sacrificar volumen de informaci√≥n.

## Justificaci√≥n num√©rica:

Haciendo limpieza de columnas cr√≠ticas los resultados son mejores: 
* Cantidad de filas originales: 100
* Cantidad de filas despu√©s de limpieza: 93

## Funci√≥n detect_suspicious_transactions(df):

* Recibe el DataFrame limpio y detecta transacciones sospechosas seg√∫n criterios definidos.

**Criterios principales:**

* Montos inusualmente altos (percentil 99).
* M√∫ltiples intentos fallidos del mismo usuario.
* Flaggeo de transacciones declined con c√≥digos de seguridad (usando tanto response_message que contenga ‚Äúsecurity‚Äù como response_code igual a 65).
* Patrones an√≥malos (transacciones r√°pidas, horarios inusuales).
* Transacciones internacionales de alto riesgo.

**Nota:**
Lo m√©todos estad√≠sticos aplicados y las regflas basadas en patrones t√≠picos cumplen con lo solicitado. 
Para alcanzar un nivel de optimizaci√≥n superior y una calibraci√≥n din√°mica de los criterios, ser√≠a recomendable desarrollar y evaluar un modelo de machine learning supervisado.
No obstante, este procedimiento excede el alcance y las instrucciones del reto actual, por lo que la soluci√≥n presentada prioriza robustez y flexibilidad mediante reglas basadas en el an√°lisis estad√≠stico de los datos.

* La funci√≥n retorna dos DataFrames:
**normal_df:** transacciones normales, que se guardan en la carpeta **processed**. Y el DataFrame
**suspicious_df:** transacciones sospechosas, que se guardan en la carpeta **suspicious**.

## Testeo de funciones:
La carpeta scripts contiene dos archivos: 
* **test_clean.py** que prueba la funci√≥n clean_data. 
* **test_suspicious.py** que prueba la funci√≥n detect_suspicious_transactions. 

### Fase 3: Data Warehouse - Modelado y Almacenamiento
**Objetivo:** Dise√±ar e implementar un modelo dimensional para an√°lisis

* **inspect_processed.py**: Este script permite inspeccionar los archivos CSV limpios generados por el pipeline ETL y almacenados en la carpeta **processed**. Su objetivo es mostrar las columnas disponibles, los tipos de datos y la cantidad de valores nulos en los datos finales, facilitando el dise√±o del modelo dimensional y la creaci√≥n de las tablas en el Data Warehouse.

**Uso recomendado:**
Ejecuta este script antes de definir el esquema de las tablas en PostgreSQL para asegurarte de que el modelo dimensional se adapte a los datos realmente disponibles.

Comando de ejecuci√≥n:

```bash
python scripts/inspect_processed.py
```

* **test_connection.py**
Este script realiza una prueba simple de conexi√≥n con la base de datos PostgreSQL utilizando SQLAlchemy.
Permite verificar r√°pidamente que las credenciales, el servidor y la base de datos est√°n configurados correctamente antes de ejecutar cualquier proceso de carga o consulta.

Uso recomendado:
1. Antes de ejecutar, aseg√∫rate de tener PostgreSQL instalado y en ejecuci√≥n.
2. Crea un archivo `.env` en la ra√≠z del proyecto (no incluido en el repositorio) con tus credenciales:
DB_USER=tu_usuario
DB_PASS=tu_contrase√±a
DB_NAME=tu_base_de_datos
DB_HOST=localhost
DB_PORT=5432
3. Edita `test_connection.py` para reemplazar la contrase√±a por la tuya.

5. Salida esperada:

‚úÖ Conexi√≥n exitosa a PostgreSQL

![Conexi√≥n exitosa](./docs/test_connection.jpg)

Comando de ejecuci√≥n:

```bash
python scripts/test_connection.py
```

### **Selecci√≥n de columnas y criterio de modelado**
El modelo dimensional fue dise√±ado a partir de la inspecci√≥n de los archivos CSV limpios generados por el pipeline ETL. Se priorizaron las columnas que aportan valor anal√≠tico y permiten responder preguntas de negocio relevantes, como usuario, comercio, m√©todo de pago, tiempo, montos y m√©tricas transaccionales.

Las columnas seleccionadas para cada tabla se eligieron siguiendo estos criterios:

* Evitar redundancia: Los identificadores y atributos repetidos se ubicaron en dimensiones.
* Facilitar el an√°lisis: Se incluyeron m√©tricas y atributos clave en la tabla de hechos.
* Adaptaci√≥n a los datos reales: El modelo se basa en las columnas realmente disponibles tras la limpieza.

* **Modelo estrella (Star Schema)**
Se implement√≥ un esquema estrella, est√°ndar en Data Warehousing, que permite consultas r√°pidas y flexibles para an√°lisis OLAP y BI. El modelo consta de:

**Tabla de hechos** **(fact_transactions):** Centraliza las transacciones y almacena m√©tricas clave, referenciando las dimensiones mediante claves for√°neas.
*Dimensiones:
**dim_users:** Informaci√≥n relevante del usuario (pa√≠s, dispositivo, IP, agente).
**dim_merchants:** Datos del comercio (categor√≠a, pa√≠s).
**dim_payment_methods:** Tipo y proveedor de m√©todo de pago.
**dim_time:** Detalles temporales y fecha de liquidaci√≥n.

**Script de creaci√≥n de tablas**

El script create_tables.py utiliza SQLAlchemy para definir y crear autom√°ticamente la estructura de las tablas en PostgreSQL. Solo necesitas ejecutarlo una vez para preparar la base de datos antes de cargar los datos.

Comando de ejecuci√≥n:

```bash
python scripts/create_tables.py
```

Si la ejecuci√≥n es exitosa, ver√°s el mensaje:

Tablas creadas correctamente.

**Ventajas del enfoque**

* **Reproducibilidad:** Cualquier usuario puede levantar la estructura ejecutando el script.
**Flexibilidad:** El modelo se adapta a los datos finales y permite an√°lisis por usuario, comercio, m√©todo de pago y tiempo.
* **Buenas pr√°cticas:** El uso de SQLAlchemy facilita futuras migraciones y mantenimiento.

**Nota:**
Recuerda modificar la contrase√±a en la cadena de conexi√≥n del script antes de ejecutarlo.