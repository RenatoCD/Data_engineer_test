# Pipeline de datos / Data Engineering 

## Evaluaci√≥n T√©cnica Integral

**Autor:** [Yuri D√≠az]  

### Descripci√≥n

El objetivo de este repositorio es dise√±ar e implementar un pipeline de datos end-to-end para una fintech latinoamericana, abarcando desde la ingesta y limpieza de datos hasta el modelado dimensional y la detecci√≥n de fraude.

A continuaci√≥n se documentan las fases completadas, las decisiones t√©cnicas tomadas y el razonamiento detr√°s de cada etapa del desarrollo.

---

# Preparaci√≥n de entorno virtual:


Crear un entorno virtual de trabajo utilizando los siguientes comandos:

**py -m venv venv** 

 Otras opciones en lugar de **py** pueden ser **python** o **python3.**

Activamos el entorno con uno de estos tres posibles comandos:

1. source venv/Scripts/activate
2. source venv\Scripts\activate
3. source venv/bin/activate | Linux y Mac


## Instalaci√≥n de dependencias
```bash
pip install -r requirements.txt
```
# Fase 1: Data Ingestion (Generaci√≥n y Organizaci√≥n de Datos)

## üìã Descripci√≥n general

En esta primera fase se configur√≥ la base del *Data Lake local* y se ejecut√≥ el proceso de generaci√≥n autom√°tica de transacciones.  
El objetivo fue establecer una estructura organizada para la ingesta continua de datos y preparar el entorno para las siguientes fases del pipeline ETL.

El archivo `main.py` se encarga de generar archivos CSV con transacciones simuladas cada 60 segundos dentro de la carpeta `transactions/`.

Durante esta fase:
- Se ejecut√≥ `main.py` para verificar la creaci√≥n autom√°tica de los archivos CSV.

## Ejecuci√≥n
```bash
python3 main.py
```

- Se verific√≥ que los archivos CSV fueran generados correctamente por main.py y que su estructura (columnas y formato) fuera consistente entre los distintos archivos.


### üìã Inspecci√≥n general de archivos

El script `inspect_transactions.py` permite analizar los archivos CSV generados en la carpeta `transactions/`.  
Este script realiza validaciones b√°sicas, como verificar la consistencia de las columnas y detectar posibles errores en los datos.

### Ejecuci√≥n
```bash
python3 inspect_transactions.py
```

- El script genera un reporte con los resultados de la inspecci√≥n, indicando si los archivos cumplen con los est√°ndares definidos.


# Fase 2. ETL Pipeline - Limpieza y Detecci√≥n de Fraude

Creaci√≥n y aplicaci√≥n de funciones:

## Funci√≥n clean_data(df): 
* Limpia y valida el DataFrame de transacciones crudas.
* Elimina valores nulos y duplicados en las columnas cr√≠ticas.
* Estandariza formatos de texto (may√∫sculas, sin espacios) en campos clave como currency, pa√≠s, etc.
* Convierte tipos de datos (num√©ricos y fechas) correctamente.
* Maneja outliers en la columna amount usando el m√©todo IQR.
* Prepara los datos para la detecci√≥n de fraude, pero no guarda el DataFrame limpio directamente; lo pasa a la siguiente etapa del pipeline.

**Decisi√≥n sobre la limpieza de datos**.
Durante la fase de limpieza de datos, se evaluaron dos enfoques:

**Limpieza de todas las columnas:** 

 **Ventaja:** Garantiza m√°xima calidad y consistencia en todos los campos.

**Desventaja:** Puede eliminar una gran cantidad de registros por valores faltantes o inconsistentes en columnas poco relevantes, reduciendo el volumen de datos √∫til para an√°lisis y detecci√≥n de fraude.

Datos totales despu√©s de limpieza:

* Cantidad de filas originales: 100
* Cantidad de filas despu√©s de limpieza: 54


**Limpieza solo de columnas cr√≠ticas:**

**Ventaja:** Preserva m√°s registros, enfoc√°ndose en los campos esenciales para el an√°lisis y la detecci√≥n de transacciones sospechosas.

**Desventaja:** Puede dejar inconsistencias menores en columnas no cr√≠ticas, pero no afecta la calidad del an√°lisis principal.

**Decisi√≥n tomada:**
Se opt√≥ por limpiar √∫nicamente las columnas cr√≠ticas para el proceso de an√°lisis y detecci√≥n de fraude:

* transaction_id
* user_id
* merchant_id
* amount
* currency
* status
* timestamp
* payment_method
* country

Esta decisi√≥n permite mantener la mayor cantidad de datos posible, asegurando la calidad en los campos relevantes y facilitando la detecci√≥n de patrones sospechosos sin sacrificar volumen de informaci√≥n.

## Justificaci√≥n num√©rica:

Haciendo limpieza de columnas cr√≠ticas los resultados son mejores: 
* Cantidad de filas originales: 100
* Cantidad de filas despu√©s de limpieza: 93

## Funci√≥n detect_suspicious_transactions(df):

* Recibe el DataFrame limpio y detecta transacciones sospechosas seg√∫n criterios definidos.

**Criterios principales:**

* Montos inusualmente altos (percentil 99).
* M√∫ltiples intentos fallidos del mismo usuario.
* Flaggeo de transacciones declined con c√≥digos de seguridad (usando tanto response_message que contenga ‚Äúsecurity‚Äù como response_code igual a 65).
* Patrones an√≥malos (transacciones r√°pidas, horarios inusuales).
* Transacciones internacionales de alto riesgo.

**Nota:**
Lo m√©todos estad√≠sticos aplicados y las regflas basadas en patrones t√≠picos cumplen con lo solicitado. 
Para alcanzar un nivel de optimizaci√≥n superior y una calibraci√≥n din√°mica de los criterios, ser√≠a recomendable desarrollar y evaluar un modelo de machine learning supervisado.
No obstante, este procedimiento excede el alcance y las instrucciones del reto actual, por lo que la soluci√≥n presentada prioriza robustez y flexibilidad mediante reglas basadas en el an√°lisis estad√≠stico de los datos.

* La funci√≥n retorna dos DataFrames:
**normal_df:** transacciones normales, que se guardan en la carpeta **processed**. Y el DataFrame
**suspicious_df:** transacciones sospechosas, que se guardan en la carpeta **suspicious**.

## Testeo de funciones:
La carpeta scripts contiene dos archivos: 
* **test_clean.py** que prueba la funci√≥n clean_data. 
* **test_suspicious.py** que prueba la funci√≥n detect_suspicious_transactions. 

### Fase 3: Data Warehouse - Modelado y Almacenamiento
**Objetivo:** Dise√±ar e implementar un modelo dimensional para an√°lisis

* **inspect_processed.py**: Este script permite inspeccionar los archivos CSV limpios generados por el pipeline ETL y almacenados en la carpeta **processed**. Su objetivo es mostrar las columnas disponibles, los tipos de datos y la cantidad de valores nulos en los datos finales, facilitando el dise√±o del modelo dimensional y la creaci√≥n de las tablas en el Data Warehouse.

**Uso recomendado:**
Ejecuta este script antes de definir el esquema de las tablas en PostgreSQL para asegurarte de que el modelo dimensional se adapte a los datos realmente disponibles.

Comando de ejecuci√≥n:

```bash
python scripts/inspect_processed.py
```

* **test_connection.py**
Este script realiza una prueba simple de conexi√≥n con la base de datos PostgreSQL utilizando SQLAlchemy.
Permite verificar r√°pidamente que las credenciales, el servidor y la base de datos est√°n configurados correctamente antes de ejecutar cualquier proceso de carga o consulta.

Uso recomendado:
1. Antes de ejecutar, aseg√∫rate de tener PostgreSQL instalado y en ejecuci√≥n.
2. Crea un archivo `.env` en la ra√≠z del proyecto (no incluido en el repositorio) con tus credenciales:
DB_USER=tu_usuario
DB_PASS=tu_contrase√±a
DB_NAME=tu_base_de_datos
DB_HOST=localhost
DB_PORT=5432
3. Edita `test_connection.py` para reemplazar la contrase√±a por la tuya.

5. Salida esperada:

‚úÖ Conexi√≥n exitosa a PostgreSQL

![Conexi√≥n exitosa](./docs/test_connection.jpg)

Comando de ejecuci√≥n:

```bash
python scripts/test_connection.py
```

### **Criterio del modelado**

**Descripci√≥n**

Este proyecto implementa un Data Warehouse para transacciones fintech usando PostgreSQL y SQLAlchemy.

Las columnas seleccionadas para cada tabla se eligieron siguiendo estos criterios:

* Evitar redundancia: Los identificadores y atributos repetidos se ubicaron en dimensiones.
* Facilitar el an√°lisis: Se incluyeron m√©tricas y atributos clave en la tabla de hechos.
* Adaptaci√≥n a los datos reales: El modelo se basa en las columnas realmente disponibles tras la limpieza.


Se utiliza un **modelo estrella (star schema)** con una **tabla de hechos** (fact_transactions) y **dimensiones** (dim_users, dim_merchants, dim_payment_methods, dim_time) para facilitar an√°lisis OLAP y detecci√≥n de fraude.

### Modelo Estrella

**Tabla de hechos** (fact_transactions): almacena cada transacci√≥n con m√©tricas y claves a dimensiones.

| Columna           | Tipo                               | Descripci√≥n                |
| ----------------- | ---------------------------------- | -------------------------- |
| transaction_id    | String (PK)                        | ID √∫nico de la transacci√≥n |
| user_id           | Integer (FK ‚Üí dim_users)           | Usuario asociado           |
| merchant_id       | Integer (FK ‚Üí dim_merchants)       | Comercio asociado          |
| payment_method_id | Integer (FK ‚Üí dim_payment_methods) | M√©todo de pago             |
| time_id           | Integer (FK ‚Üí dim_time)            | Referencia temporal        |
| amount            | Float                              | Monto de la transacci√≥n    |
| currency          | String                             | Moneda                     |
| status            | String                             | Estado de la transacci√≥n   |
| response_message  | String                             | Mensaje del procesador     |
| attempt_number    | Integer                            | N√∫mero de intentos         |


**Dimensiones:** describen atributos de usuarios, comercios, m√©todos de pago y tiempo.

Permite consultas r√°pidas y agregaciones: monto total por usuario, transacciones por d√≠a, an√°lisis de intentos fallidos, etc.

Cada dimensi√≥n refleja √∫nicamente los atributos existentes en los datos finales:

* **dim_users ‚Üí user_id, country**

* **dim_merchants ‚Üí merchant_id, country**

* **dim_payment_methods ‚Üí payment_method**

* **dim_time** ‚Üí descomposici√≥n de timestamp en year, month, day, hour, minute, second y settlement_date (calculada durante la carga)

**Script de creaci√≥n de tablas**

El script **create_tables.py** utiliza SQLAlchemy para definir y crear autom√°ticamente la estructura de las tablas en PostgreSQL. Solo necesitas ejecutarlo una vez para preparar la base de datos antes de cargar los datos.

Comando de ejecuci√≥n:

```bash
python scripts/create_tables.py
```

Si la ejecuci√≥n falla saldr√° un mensaje de error y si es exitosa, ver√°s el mensaje:

"Tablas creadas correctamente."

![Conexi√≥n exitosa](./docs/create_table_test.jpg)

Y en PostgreSQL ahora aparecen las tablas listadas:

![Conexi√≥n exitosa](./docs/empty_tables.jpg)


**Ventajas del enfoque**

* **Reproducibilidad:** Cualquier usuario puede levantar la estructura ejecutando el script.
**Flexibilidad:** El modelo se adapta a los datos finales y permite an√°lisis por usuario, comercio, m√©todo de pago y tiempo.
* **Buenas pr√°cticas:** El uso de SQLAlchemy facilita futuras migraciones y mantenimiento.

**Nota:**
Recuerda modificar la contrase√±a en la cadena de conexi√≥n del script antes de ejecutarlo.

**Script de carga de datos**

El script **load_to_postgres.py** utiliza SQLAlchemy para cargar autom√°ticamente los datos limpios desde la carpeta `processed/` en las tablas del Data Warehouse en PostgreSQL. El proceso recorre cada registro del archivo CSV m√°s reciente, valida y transforma los datos seg√∫n el modelo dimensional, y los inserta en las tablas correspondientes (dimensiones y hechos).

Solo necesitas ejecutarlo despu√©s de haber creado las tablas con `create_tables.py`. El script gestiona la inserci√≥n evitando duplicados en las dimensiones y asegurando la integridad referencial entre las tablas.

Comando de ejecuci√≥n:

```bash
python scripts/load_to_postgres.py
```

Si la ejecuci√≥n es exitosa, ver√°s el mensaje:

"Datos cargados correctamente."

![Conexi√≥n exitosa](./docs/data_loaded_terminal.jpg)

Y en Postgres estar√°n cargados los datos en las tablas:

![Conexi√≥n exitosa](./docs/data_loaded.jpg)

En caso de errores (por ejemplo, problemas de conexi√≥n o datos inv√°lidos), se mostrar√° un mensaje explicativo para facilitar la depuraci√≥n.

**Configuraci√≥n y recomendaciones:**

* Antes de ejecutar, aseg√∫rate de que PostgreSQL est√© instalado y en funcionamiento.
* Verifica que la cadena de conexi√≥n en el script tenga tus credenciales correctas.
* Si el script detecta un `timestamp` inv√°lido en alguna fila, esa fila ser√° ignorada y se mostrar√° un mensaje indicando el `transaction_id` afectado.
* El proceso garantiza que solo se inserten registros v√°lidos y consistentes con el modelo dimensional.

**Ventajas del enfoque**

* **Automatizaci√≥n:** El proceso de carga es reproducible y no requiere intervenci√≥n manual.
* **Integridad:** Solo se insertan registros v√°lidos y consistentes con el modelo dimensional.
* **Escalabilidad:** El script est√° optimizado para manejar grandes vol√∫menes de datos de manera eficiente.
